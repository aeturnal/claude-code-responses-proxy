# Milestone v1.0: MVP Compatibility

**Status:** ✅ SHIPPED 2026-01-26
**Phases:** 1-6
**Total Plans:** 12

## Overview

Deliver a production-grade compatibility proxy that preserves Anthropic Messages semantics on top of OpenAI Responses. The v1.0 roadmap focuses on core request/response parity, token counting alignment, privacy-first observability, and streaming/tool-use parity so Claude Code can run without semantic breaks.

## Phases

### Phase 1: Core Messages Parity

**Goal**: Users can call `/v1/messages` and receive Anthropic-compatible responses and errors.
**Depends on**: Nothing (first phase)
**Plans**: 4 plans

Plans:

- [x] 01-01: Scaffold dependencies, config, and OpenAI transport
- [x] 01-02: Define request schemas + Anthropic→OpenAI mapping
- [x] 01-03: Normalize OpenAI responses + stop_reason derivation
- [x] 01-04: Implement /v1/messages handler + error envelopes

**Details:**
- **Requirements**: CORE-01, CORE-02, CORE-03
- **Success Criteria:**
  1. User can send an Anthropic-style `/v1/messages` request and receive a valid Anthropic response shape.
  2. Response `stop_reason` matches Anthropic semantics for typical completion and stop conditions.
  3. When upstream errors occur, user receives a deterministic Anthropic-style error envelope with OpenAI details included.

### Phase 2: Token Counting Alignment

**Goal**: Users can preflight token usage with Anthropic-compatible counting aligned to OpenAI billing.
**Depends on**: Phase 1
**Plans**: 1 plan

Plans:

- [x] 02-01: Implement token counting utilities and /v1/messages/count_tokens endpoint

**Details:**
- **Requirements**: TOK-01, TOK-02
- **Success Criteria:**
  1. User can call `/v1/messages/count_tokens` and receive token counts aligned to OpenAI billing behavior.
  2. System and tool content in the request are counted consistently with the mapping used by `/v1/messages`.

### Phase 3: Privacy-First Observability

**Goal**: Operators can observe requests and responses without exposing PII.
**Depends on**: Phase 1
**Plans**: 2 plans

Plans:

- [x] 03-01: Add observability config, logging setup, and correlation ID middleware
- [x] 03-02: Redaction utilities + request/response logging with correlation propagation

**Details:**
- **Requirements**: OBS-01, OBS-02
- **Success Criteria:**
  1. Operator can see structured logs for requests and responses with PII redacted by default.
  2. Each request/response log entry includes a correlation ID that can be used to trace a single interaction.

### Phase 4: Streaming + Tool Use Parity

**Goal**: Users can stream Anthropic-compatible SSE events with tool_use and input_json_delta parity.
**Depends on**: Phase 1, Phase 3
**Plans**: 3 plans

Plans:

- [x] 04-01: Add OpenAI streaming transport + SSE parsing
- [x] 04-02: Translate OpenAI stream events to Anthropic SSE + tool_use deltas
- [x] 04-03: Implement /v1/messages/stream endpoint + logging/error handling

**Details:**
- **Requirements**: STREAM-01, STREAM-02, STREAM-03
- **Success Criteria:**
  1. User can open `/v1/messages/stream` and receive Anthropic-compatible message/content block lifecycle events in order.
  2. Tool use blocks stream in Anthropic format and complete tool blocks are emitted at block stop.
  3. `input_json_delta` events are accumulated during streaming and appear as finalized input JSON in tool blocks.

### Phase 5: Credential Error Envelope Parity

**Goal**: Missing OpenAI credentials return Anthropic error envelopes for both messages and streaming.
**Depends on**: Phase 1, Phase 4
**Plans**: 1 plan

Plans:

- [x] 05-01: Add missing-credential error envelope handling + tests

**Details:**
- **Requirements**: CORE-03 (error envelope parity)
- **Gap Closure**: Missing `OPENAI_API_KEY` error shape parity
- **Success Criteria:**
  1. `/v1/messages` with no `OPENAI_API_KEY` returns Anthropic error envelope (not 500).
  2. `/v1/messages/stream` with no `OPENAI_API_KEY` emits `event: error` with Anthropic error envelope.
  3. Tests cover missing-credential error mapping for both paths.

### Phase 6: Token Count Billing Alignment Verification

**Goal**: Verify `/v1/messages/count_tokens` outputs match OpenAI billing semantics for mapped requests.
**Depends on**: Phase 2
**Plans**: 1 plan

Plans:

- [x] 06-01: Add token count alignment verification harness + docs

**Details:**
- **Requirements**: TOK-01
- **Gap Closure**: Token-count alignment verification evidence
- **Success Criteria:**
  1. Verification script or harness compares proxy counts against OpenAI billing counts.
  2. Verification steps documented with sample inputs/outputs.
  3. Phase 2 verification updated to reflect completed alignment check.

---

## Milestone Summary

**Decimal Phases:**

- None in this milestone.

**Key Decisions:**

- Anthropic → OpenAI translation only — Implemented for v1.0 parity
- Explicit model mapping with env overrides — Implemented for v1.0 parity
- Hybrid error shape (Anthropic envelope + OpenAI details) — Implemented for v1.0 parity
- Local disk file storage — Deferred (Files API is post‑MVP)
- MVP excludes size limits/rate limiting — Maintained exclusion for v1.0

**Issues Resolved:**

- Stabilized tool_use streaming metadata and input parsing during live SSE.
- Accepted tool_use blocks in request history to avoid upstream rejection.
- Added missing FunctionDefinition schema for tool payload compatibility in tests.
- Hardened verification harness imports for local execution.

**Issues Deferred:**

- None noted.

**Technical Debt Incurred:**

- None noted.

---

_For current project status, see .planning/ROADMAP.md_
