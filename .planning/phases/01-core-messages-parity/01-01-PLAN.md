---
phase: 01-core-messages-parity
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/__init__.py
  - src/config.py
  - src/transport/__init__.py
  - src/transport/openai_client.py
autonomous: true
user_setup:
  - service: openai
    why: "Upstream Responses API access"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API keys"
must_haves:
  truths:
    - "OpenAI Responses requests can be issued using configured credentials"
    - "Anthropic model names are deterministically resolved to an OpenAI model"
  artifacts:
    - path: "requirements.txt"
      provides: "FastAPI/httpx runtime dependencies"
      contains: "fastapi"
    - path: "src/config.py"
      provides: "OpenAI config + model resolution"
      contains: "resolve_openai_model"
    - path: "src/transport/openai_client.py"
      provides: "HTTP client for /v1/responses"
      exports: ["create_openai_response", "OpenAIUpstreamError"]
  key_links:
    - from: "src/transport/openai_client.py"
      to: "src/config.py"
      via: "base URL + API key resolution"
      pattern: "from src\.config import .*OPENAI"
---

<objective>
Establish the Python scaffold, configuration, and OpenAI Responses transport needed to call /v1/responses.

Purpose: Provide stable foundations for request/response mapping and endpoint handling.
Output: Dependency list, config helpers, and HTTP client for OpenAI Responses.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-messages-parity/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Python dependency scaffold</name>
  <files>requirements.txt, src/__init__.py, src/transport/__init__.py</files>
  <action>
    - Create requirements.txt with: fastapi, uvicorn, httpx, pydantic, pytest.
    - Create src/__init__.py and src/transport/__init__.py to mark packages.
  </action>
  <verify>python -m pip install -r requirements.txt</verify>
  <done>Dependencies install cleanly and package folders import without errors.</done>
</task>

<task type="auto">
  <name>Task 2: Implement config + OpenAI transport client</name>
  <files>src/config.py, src/transport/openai_client.py</files>
  <action>
    - In src/config.py, load OPENAI_API_KEY (required), OPENAI_BASE_URL (default https://api.openai.com/v1), OPENAI_DEFAULT_MODEL (default gpt-4.1), and optional MODEL_MAP_JSON (JSON dict). Provide resolve_openai_model(anthropic_model) to use MODEL_MAP_JSON override or default.
    - In src/transport/openai_client.py, create OpenAIUpstreamError(status_code, error_payload) and create_openai_response(payload) using httpx.AsyncClient to POST {OPENAI_BASE_URL}/responses with Authorization header.
    - On non-2xx, parse JSON error payload when possible and raise OpenAIUpstreamError with status + payload for handler-level wrapping.
  </action>
  <verify>python -c "from src.transport.openai_client import OpenAIUpstreamError, create_openai_response; print('ok')"</verify>
  <done>Config resolves model/base URL and transport raises OpenAIUpstreamError on upstream failure.</done>
</task>

</tasks>

<verification>
- requirements.txt installs successfully.
- OpenAI transport module imports and exposes create_openai_response.
</verification>

<success_criteria>
- Config exposes deterministic model resolution and API base/secret handling.
- OpenAI transport is ready for /v1/responses calls with structured error capture.
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-messages-parity/01-01-SUMMARY.md`
</output>
