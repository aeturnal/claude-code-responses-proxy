---
phase: 02-token-counting-alignment
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/token_counting/openai_count.py
  - src/handlers/count_tokens.py
  - src/schema/anthropic.py
  - src/app.py
  - tests/test_token_counting.py
autonomous: true
must_haves:
  truths:
    - "User can call /v1/messages/count_tokens and receive an input_tokens integer."
    - "System, messages, and tool definitions are counted using the same normalization as /v1/messages."
    - "Unknown models still return a count using a safe fallback encoding."
  artifacts:
    - path: "src/token_counting/openai_count.py"
      provides: "OpenAI-aligned token counting utilities"
    - path: "src/handlers/count_tokens.py"
      provides: "POST /v1/messages/count_tokens handler"
      exports: ["router"]
    - path: "src/schema/anthropic.py"
      provides: "CountTokensResponse schema"
      contains: "class CountTokensResponse"
    - path: "tests/test_token_counting.py"
      provides: "token counting coverage"
  key_links:
    - from: "src/handlers/count_tokens.py"
      to: "src/mapping/anthropic_to_openai.py"
      via: "map_anthropic_request_to_openai"
      pattern: "map_anthropic_request_to_openai"
    - from: "src/handlers/count_tokens.py"
      to: "src/token_counting/openai_count.py"
      via: "count_openai_request_tokens"
      pattern: "count_openai_request_tokens"
    - from: "src/app.py"
      to: "src/handlers/count_tokens.py"
      via: "include_router"
      pattern: "include_router\(count_tokens_router\)"
    - from: "src/token_counting/openai_count.py"
      to: "tiktoken"
      via: "encoding_for_model"
      pattern: "encoding_for_model"
---

<objective>
Implement token counting aligned to OpenAI billing and expose /v1/messages/count_tokens.

Purpose: Let users preflight token usage with the same normalization used by /v1/messages.
Output: Token counting utilities, endpoint handler, and tests.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-token-counting-alignment/02-RESEARCH.md

@src/handlers/messages.py
@src/mapping/anthropic_to_openai.py
@src/schema/anthropic.py
@src/schema/openai.py
@src/app.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add OpenAI-aligned token counting utilities + tests</name>
  <files>requirements.txt, src/token_counting/openai_count.py, tests/test_token_counting.py</files>
  <action>
    - Add `tiktoken` to requirements.txt.
    - Create src/token_counting/openai_count.py with helpers that count input tokens for an OpenAIResponsesRequest:
      - `get_encoding(model: str)` uses tiktoken.encoding_for_model() with fallback to "o200k_base" when unknown.
      - `count_message_tokens(messages, model)` follows OpenAI cookbook: tokens_per_message=3, tokens_per_name=1 for known snapshots (gpt-4o/gpt-4o-mini/0613-style), adds reply primer +3.
      - Convert OpenAI input items into a message list by joining InputTextItem.text with "\n" per message item.
      - If `instructions` is present, treat it as a leading system message so instructions are counted.
      - `count_tool_tokens(tools, model)` counts tool definitions using the OpenAI cookbook approach: encode name/description/parameters JSON and add fixed overhead per tool; apply 0 when no tools.
      - `count_openai_request_tokens(request)` combines message + instruction + tool counts and returns an int.
    - Add tests in tests/test_token_counting.py that:
      - Build a minimal OpenAIResponsesRequest (or dict) with one user message and assert the count equals computed encoding length + overhead constants (compute expected using the same encoding in test).
      - Include instructions and ensure count increases by the instruction token length + overhead.
      - Include a tool definition and assert count increases vs the no-tools case.
      - Use a fake model name and assert fallback encoding path still returns a count.
  </action>
  <verify>pytest -k token_counting</verify>
  <done>Token counting helpers exist, pass tests, and include instructions/tools in counts.</done>
</task>

<task type="auto">
  <name>Task 2: Implement /v1/messages/count_tokens endpoint</name>
  <files>src/handlers/count_tokens.py, src/schema/anthropic.py, src/app.py</files>
  <action>
    - Add `CountTokensResponse` to src/schema/anthropic.py with `input_tokens: int`.
    - Create src/handlers/count_tokens.py with a FastAPI router:
      - POST /v1/messages/count_tokens accepts MessagesRequest.
      - Reuse map_anthropic_request_to_openai to normalize the payload.
      - Call count_openai_request_tokens and return CountTokensResponse.
      - On ValueError from mapping/token counting, return an Anthropic error envelope using build_anthropic_error with 400/invalid_request_error.
    - Wire the router in src/app.py (import as count_tokens_router) and include_router.
  </action>
  <verify>pytest -k token_counting</verify>
  <done>Endpoint returns input_tokens and uses shared normalization with consistent errors.</done>
</task>

</tasks>

<verification>
- pytest -k token_counting
</verification>

<success_criteria>
- /v1/messages/count_tokens responds with input_tokens for valid requests.
- Counts include system instructions and tool definitions consistent with /v1/messages mapping.
- Tests cover basic, instructions, tools, and unknown-model fallback paths.
</success_criteria>

<output>
After completion, create `.planning/phases/02-token-counting-alignment/02-01-SUMMARY.md`
</output>
